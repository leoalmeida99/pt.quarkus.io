msgid ""
msgstr ""
"Language: pt_BR\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"X-Generator: jekyll-l10n\n"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Creating pure Java LLM infused application with Quarkus, Langchain4j and Jlama"
msgstr "Criação de aplicativos Java LLM puros com Quarkus, Langchain4j e Jlama"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Creating pure Java LLM infused application with Quarkus, LangChain4j and Jlama"
msgstr "Criação de um aplicativo Java puro com LLM com Quarkus, LangChain4j e Jlama"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Currently the vast majority of LLM-based applications rely on external services provided by specialized companies. These services typically offer the access to huge, general purpose models, implying energy consumption and then costs that are proportional to the size of these models."
msgstr "Atualmente, a grande maioria dos aplicativos baseados em LLM depende de serviços externos fornecidos por empresas especializadas. Esses serviços normalmente oferecem acesso a modelos enormes de uso geral, o que implica em consumo de energia e custos proporcionais ao tamanho desses modelos."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Even worse, this usage pattern also comes with both privacy and security concerns, since it is virtually impossible to be sure how those service providers will eventually re-use the prompts of their customers, which in some cases could also contain sensitive information."
msgstr "Pior ainda, esse padrão de uso também traz preocupações com a privacidade e a segurança, pois é praticamente impossível ter certeza de como esses provedores de serviços reutilizarão os prompts de seus clientes, que, em alguns casos, também podem conter informações confidenciais."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "For these reasons many companies are exploring the option of training or fine-tuning smaller models that do not claim to be usable in a general context, but that are tailored towards specific business needs and subsequently running (serving in LLM terms) these models on premise or on private clouds."
msgstr "Por esses motivos, muitas empresas estão explorando a opção de treinar ou ajustar modelos menores que não pretendem ser utilizáveis em um contexto geral, mas que são adaptados a necessidades comerciais específicas e, posteriormente, executar (servir em termos de LLM) esses modelos no local ou em nuvens privadas."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The features provided by these specialized models need to be integrated into the existing software infrastructure, that in the enterprise world are very often written in Java. This could be accomplished following a traditional client-server architecture, for instance serving the model through an external server like https://ollama.com/[Ollama] and querying it through REST calls. While this should not present any particular problem for Java developers, they could work more efficiently, if they could consume the model directly in Java and without any need to install additional tools. Finally the possibility of embedding the LLM interaction directly in the same Java process running the application will make it easier to move from local dev to deployment, relieving IT from the burden of managing an external server, thus bypassing the need for a more mature platform engineering strategy. This is where Jlama comes into play."
msgstr "Os recursos fornecidos por esses modelos especializados precisam ser integrados à infraestrutura de software existente, que, no mundo corporativo, muitas vezes é escrita em Java. Isso poderia ser feito seguindo uma arquitetura cliente-servidor tradicional, por exemplo, servindo o modelo por meio de um servidor externo como o link:https://ollama.com/[Ollama] e consultando-o por meio de chamadas REST. Embora isso não deva representar nenhum problema específico para os desenvolvedores de Java, eles poderiam trabalhar com mais eficiência se pudessem consumir o modelo diretamente em Java e sem a necessidade de instalar ferramentas adicionais. Por fim, a possibilidade de incorporar a interação do LLM diretamente no mesmo processo Java que executa o aplicativo facilitará a passagem do desenvolvimento local para a implementação, aliviando a TI do ônus de gerenciar um servidor externo, evitando assim a necessidade de uma estratégia de engenharia de plataforma mais madura. É aqui que a Jlama entra em ação."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "How and why executing LLM inference in pure Java with Jlama"
msgstr "Como e por que executar a inferência LLM em Java puro com o Jlama"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "https://github.com/tjake/Jlama[Jlama] is a library allowing to execute LLM inference in pure Java. It supports many LLM model families like Llama, Mistral, Qwen2 and Granite. It also implements out-of-the-box many useful LLM related features like functions calling, models quantization, mixture of experts and even distributed inference."
msgstr "link:https://github.com/tjake/Jlama[Jlama] é uma biblioteca que permite executar a inferência LLM em Java puro. Ela é compatível com muitas famílias de modelos LLM, como Llama, Mistral, Qwen2 e Granite. Ela também implementa, de imediato, muitos recursos úteis relacionados ao LLM, como chamada de funções, quantização de modelos, mistura de especialistas e até mesmo inferência distribuída."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Jlama is well integrated with Quarkus through the https://quarkus.io/extensions/io.quarkiverse.langchain4j/quarkus-langchain4j-jlama/[dedicated lanchain4j based extension]. Note that for performance reasons Jlama uses the https://openjdk.org/jeps/469[Vector API] which is still in preview in Java 23, and very likely will be released as a supported feature in Java 25."
msgstr "A Jlama está bem integrada ao Quarkus por meio da link:https://quarkus.io/extensions/io.quarkiverse.langchain4j/quarkus-langchain4j-jlama/[extensão dedicada baseada em lanchain4j] . Observe que, por motivos de desempenho, a Jlama usa a link:https://openjdk.org/jeps/469[API Vector] , que ainda está em visualização no Java 23 e muito provavelmente será lançada como um recurso compatível no Java 25."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "In essence Jlama makes it possible to serve an LLM in Java, directly embedded in the same JVM running your Java application, but why could this be useful? Actually this is desirable in many use cases and presents a number of relevant advantages like the following:"
msgstr "Essencialmente, o Jlama possibilita atender a um LLM em Java, diretamente incorporado na mesma JVM que executa o aplicativo Java, mas por que isso pode ser útil? Na verdade, isso é desejável em muitos casos de uso e apresenta uma série de vantagens relevantes, como as seguintes:"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Similar lifecycle between model and app*: There can be use cases where the model and the application using it have the same lifecycle, so that the development of a new feature in the application also requires a change in the model. Similarly, since prompts are very dependent on the model, when the model gets updated even through fine-tuning, your prompt may need to be replaced. In these situations having the model embedded in the application will contribute to simplify the versioning and traceability of the development cycle."
msgstr "*Ciclo de vida semelhante entre o modelo e o aplicativo* : Pode haver casos de uso em que o modelo e o aplicativo que o utiliza tenham o mesmo ciclo de vida, de modo que o desenvolvimento de um novo recurso no aplicativo também exija uma alteração no modelo. Da mesma forma, como os prompts dependem muito do modelo, quando o modelo é atualizado, mesmo que seja por meio de ajuste fino, o prompt pode precisar ser substituído. Nessas situações, ter o modelo incorporado no aplicativo contribuirá para simplificar o controle de versão e a rastreabilidade do ciclo de desenvolvimento."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Fast development/prototyping*: Not having to install, configure and interact with an external server can make the development of a LLM-based Java application much easier."
msgstr "*Desenvolvimento/prototipagem rápida* : O fato de não precisar instalar, configurar e interagir com um servidor externo pode facilitar muito o desenvolvimento de um aplicativo Java baseado em LLM."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Easy models testing*: Running the LLM inference embedded in the JVM also makes it easier to test different models and their integration during the development phase."
msgstr "*Teste fácil de modelos* : A execução da inferência LLM incorporada na JVM também facilita o teste de diferentes modelos e sua integração durante a fase de desenvolvimento."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Security*: Performing the model inference in the same JVM instance that is run the application using it, eliminates the need of interacting with the LLM only through REST calls, thus preventing the leak of private data and allowing to enforce user authorization at a much finer grain."
msgstr "*Segurança* : A execução da inferência do modelo na mesma instância da JVM que executa o aplicativo que o utiliza elimina a necessidade de interagir com o LLM somente por meio de chamadas REST, evitando, assim, o vazamento de dados privados e permitindo a aplicação da autorização do usuário em uma granulação muito mais fina."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Monolithic applications support*: The former point will be also beneficial for users still running monolithic applications, who in this way will be also able to include LLM-based capabilities in those applications without changing their architecture or platform."
msgstr "*Suporte a aplicativos monolíticos* : O ponto anterior também será benéfico para os usuários que ainda executam aplicativos monolíticos, que, dessa forma, também poderão incluir recursos baseados em LLM nesses aplicativos sem alterar sua arquitetura ou plataforma."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Monitoring and Observability*: Running the LLM inference in pure Java will also allow simplify monitoring and observability, gathering statistics on the reliability and speed of the LLM response."
msgstr "*Monitoramento e observabilidade* : A execução da inferência LLM em Java puro também permitirá simplificar o monitoramento e a observabilidade, reunindo estatísticas sobre a confiabilidade e a velocidade da resposta LLM."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Developer Experience*: Debuggability will be simplified in the same way, allowing the Java developer to also navigate and debug the Jlama code if necessary."
msgstr "*Experiência do desenvolvedor* : A capacidade de depuração será simplificada da mesma forma, permitindo que o desenvolvedor Java também navegue e depure o código Jlama, se necessário."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Distribution*: Having the possibility to run LLM inference embedded in the same Java process will also make it possible to include the model itself into the same application package of the application using it (even though this could probably be advisable only in very specific circumstances)."
msgstr "*Distribuição* : A possibilidade de executar a inferência LLM incorporada no mesmo processo Java também possibilitará a inclusão do próprio modelo no mesmo pacote de aplicativos do aplicativo que o utiliza (embora isso provavelmente seja aconselhável apenas em circunstâncias muito específicas)."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Edge friendliness*: The possibility of implementing and deploying a self-contained LLM-capable Java application will also make it a better fit than a client/server architecture for edge environments."
msgstr "*Facilidade de uso na borda* : A possibilidade de implementar e implantar um aplicativo Java autônomo com capacidade para LLM também o tornará mais adequado do que uma arquitetura cliente/servidor para ambientes de borda."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "*Embedding of auxiliary LLMs*: Many applications, especially the ones relying on agentic AI patterns, uses many different LLMs at once. For instance a smaller LLM could be used to validate and approve the responses of the main bigger one. In this case an hybrid approach could be convenient, embedding the smaller auxiliary LLMs while keeping serving the main one through a dedicated server."
msgstr "*Incorporação de LLMs auxiliares* : Muitos aplicativos, especialmente os que dependem de padrões de IA agêntica, usam muitos LLMs diferentes ao mesmo tempo. Por exemplo, um LLM menor poderia ser usado para validar e aprovar as respostas do LLM maior principal. Nesse caso, uma abordagem híbrida pode ser conveniente, incorporando os LLMs auxiliares menores e, ao mesmo tempo, mantendo o serviço do principal por meio de um servidor dedicado."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The site summarizer: a pure Java LLM-based application"
msgstr "O resumidor de sites: um aplicativo baseado em Java LLM puro"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "To demonstrate how Quarkus, Langchain4j and Jlama make straightforward to create a pure Java LLM infused application, where the LLM inference is directly embedded in the same JVM running the application I created a https://github.com/mariofusco/site-summarizer[simple project] that uses a LLM to automatically generate the summarization of a Wikipedia page or more in general of a blog post taken from any website."
msgstr "Para demonstrar como o Quarkus, o Langchain4j e o Jlama facilitam a criação de um aplicativo Java infundido com LLM puro, em que a inferência LLM é incorporada diretamente no mesmo JVM que executa o aplicativo, criei um link:https://github.com/mariofusco/site-summarizer[projeto simples] que usa um LLM para gerar automaticamente o resumo de uma página da Wikipédia ou, de modo mais geral, de uma postagem de blog retirada de qualquer site."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "This video demonstrates how the application works."
msgstr "Este vídeo demonstra como o aplicativo funciona."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Out-of-the-box this project uses a https://huggingface.co/tjake/Llama-3.2-1B-Instruct-JQ4[small Llama-3.2 model with 4-bit quantization]. When the application is compiled for the first time the model is automatically downloaded locally by Jlama from the Huggingface repository. However it is possible to replace this model and experiment with any other one by simply editing the https://github.com/mariofusco/site-summarizer/blob/main/src/main/resources/application.properties#L4[quarkus.langchain4j.jlama.chat-model.model-name property] in the application.properties file."
msgstr "Esse projeto usa um link:https://huggingface.co/tjake/Llama-3.2-1B-Instruct-JQ4[pequeno modelo Llama-3.2 com quantização de 4 bits] . Quando o aplicativo é compilado pela primeira vez, o modelo é automaticamente baixado localmente pela Jlama a partir do repositório Huggingface. No entanto, é possível substituir esse modelo e fazer experiências com qualquer outro, bastando editar a link:https://github.com/mariofusco/site-summarizer/blob/main/src/main/resources/application.properties#L4[propriedade quarkus.langchain4j.jlama.chat-model.model-name] no arquivo application.properties."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "After having compiled and packaged the project with `mvn clean package`, the simplest way to use it is launching the jar passing as argument the URL of the web page containing the article that you want to summarize, something like:"
msgstr "Depois de compilar e empacotar o projeto com `mvn clean package` , a maneira mais simples de usá-lo é iniciar o jar passando como argumento o URL da página da Web que contém o artigo que o senhor deseja resumir, algo como:"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "that will generate an output like the following:"
msgstr "que gerará um resultado como o seguinte:"

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "Note that it is necessary to launch the JVM with a few additional arguments that enable the access to the Vector API which is still a Java preview feature, but it is internally used by Jlama to speed up the computation."
msgstr "Observe que é necessário iniciar a JVM com alguns argumentos adicionais que permitem o acesso à API Vector, que ainda é um recurso de visualização do Java, mas é usado internamente pela Jlama para acelerar o cálculo."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "As better clarified by the readme of the project, there's a dedicated operating mode to process Wikipedia pages and also the possibility to expose this service through a REST endpoint."
msgstr "Conforme melhor esclarecido no readme do projeto, há um modo operacional dedicado para processar as páginas da Wikipédia e também a possibilidade de expor esse serviço por meio de um ponto de extremidade REST."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The internal implementation of this project is relatively straightforward: after having programmatically extracted the text to be summarized from the HTML page containing it, that text is sent to Jlama to be processed via a usual Langchain4j AiService."
msgstr "A implementação interna desse projeto é relativamente simples: depois de extrair programaticamente o texto a ser resumido da página HTML que o contém, esse texto é enviado à Jlama para ser processado por meio de um AiService Langchain4j comum."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "As anticipated, despite this implementation looks identical to any other LLM inference engine integrations, in this case there isn't any remote call to an external service, but the LLM inference is performed directly inside the same JVM running the application."
msgstr "Como previsto, apesar de essa implementação parecer idêntica a qualquer outra integração do mecanismo de inferência LLM, nesse caso não há nenhuma chamada remota para um serviço externo, mas a inferência LLM é realizada diretamente dentro da mesma JVM que executa o aplicativo."

#: _posts/2024-11-29-quarkus-jlama.adoc
#, fuzzy
msgid "The combination of the 2 trends of the increasing spread of small and tailored models and the adoption of these models in the enterprise software development world will very likely promote the use of similar solutions in the near future."
msgstr "A combinação das duas tendências da crescente disseminação de modelos pequenos e personalizados e a adoção desses modelos no mundo do desenvolvimento de software empresarial provavelmente promoverá o uso de soluções semelhantes em um futuro próximo."
